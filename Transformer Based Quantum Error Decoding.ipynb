{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyJ44cCuddaV",
    "outputId": "e44998c0-fcc5-457a-b119-c382e517c898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Experiment (Separate Bit Embeddings): noise_mode = independent ===\n",
      "Epoch 1/20, Loss: 0.9463, Transformer Val Accuracy: 62.30%\n",
      "Epoch 2/20, Loss: 0.8473, Transformer Val Accuracy: 62.50%\n",
      "Epoch 3/20, Loss: 0.8275, Transformer Val Accuracy: 65.00%\n",
      "Epoch 4/20, Loss: 0.8327, Transformer Val Accuracy: 68.00%\n",
      "Epoch 5/20, Loss: 0.8337, Transformer Val Accuracy: 65.10%\n",
      "Epoch 6/20, Loss: 0.8453, Transformer Val Accuracy: 67.40%\n",
      "Epoch 7/20, Loss: 0.8142, Transformer Val Accuracy: 67.20%\n",
      "Epoch 8/20, Loss: 0.8127, Transformer Val Accuracy: 66.90%\n",
      "Epoch 9/20, Loss: 0.8108, Transformer Val Accuracy: 68.00%\n",
      "Early stopping triggered at epoch 9 (best epoch: 4, best accuracy: 68.00%)\n",
      "\n",
      "Final Transformer Val Accuracy: 68.00%\n",
      "Classical Decoder (Majority Vote) Val Accuracy: 60.40%\n",
      "\n",
      "Testing examples:\n",
      "Sample 1:\n",
      " Syndrome Sequence: ['01', '11', '11', '01', '01', '11', '10', '11', '01', '01', '00', '00', '00', '00', '11', '11', '00', '01', '01', '00']\n",
      " True Correction: 3  ( (0, 1) )\n",
      " Transformer Predicted Correction: 3  ( (0, 1) )\n",
      " Classical Predicted Correction: 3  ( (0, 1) )\n",
      "------\n",
      "Sample 2:\n",
      " Syndrome Sequence: ['00', '01', '01', '01', '11', '00', '10', '11', '10', '11', '11', '11', '00', '10', '01', '10', '10', '00', '10', '00']\n",
      " True Correction: 2  ( (1, 1) )\n",
      " Transformer Predicted Correction: 1  ( (1, 0) )\n",
      " Classical Predicted Correction: 1  ( (1, 0) )\n",
      "------\n",
      "Sample 3:\n",
      " Syndrome Sequence: ['10', '11', '11', '01', '10', '01', '01', '11', '10', '11', '10', '01', '11', '10', '01', '11', '11', '10', '11', '10']\n",
      " True Correction: 2  ( (1, 1) )\n",
      " Transformer Predicted Correction: 2  ( (1, 1) )\n",
      " Classical Predicted Correction: 2  ( (1, 1) )\n",
      "------\n",
      "Sample 4:\n",
      " Syndrome Sequence: ['00', '10', '00', '00', '11', '10', '11', '10', '10', '10', '00', '11', '10', '11', '10', '00', '01', '11', '10', '00']\n",
      " True Correction: 1  ( (1, 0) )\n",
      " Transformer Predicted Correction: 1  ( (1, 0) )\n",
      " Classical Predicted Correction: 1  ( (1, 0) )\n",
      "------\n",
      "Sample 5:\n",
      " Syndrome Sequence: ['11', '00', '01', '10', '10', '01', '11', '11', '11', '11', '01', '01', '11', '11', '00', '10', '10', '01', '01', '00']\n",
      " True Correction: 3  ( (0, 1) )\n",
      " Transformer Predicted Correction: 2  ( (1, 1) )\n",
      " Classical Predicted Correction: 2  ( (1, 1) )\n",
      "------\n",
      "\n",
      "\n",
      "=== Experiment (Separate Bit Embeddings): noise_mode = biased ===\n",
      "Epoch 1/20, Loss: 0.2228, Transformer Val Accuracy: 95.90%\n",
      "Epoch 2/20, Loss: 0.1111, Transformer Val Accuracy: 95.90%\n",
      "Epoch 3/20, Loss: 0.0936, Transformer Val Accuracy: 97.10%\n",
      "Epoch 4/20, Loss: 0.0945, Transformer Val Accuracy: 97.10%\n",
      "Epoch 5/20, Loss: 0.1394, Transformer Val Accuracy: 96.90%\n",
      "Epoch 6/20, Loss: 0.0923, Transformer Val Accuracy: 96.80%\n",
      "Epoch 7/20, Loss: 0.0894, Transformer Val Accuracy: 97.00%\n",
      "Epoch 8/20, Loss: 0.0845, Transformer Val Accuracy: 97.10%\n",
      "Early stopping triggered at epoch 8 (best epoch: 3, best accuracy: 97.10%)\n",
      "\n",
      "Final Transformer Val Accuracy: 97.10%\n",
      "Classical Decoder (Majority Vote) Val Accuracy: 96.60%\n",
      "\n",
      "Testing examples:\n",
      "Sample 1:\n",
      " Syndrome Sequence: ['01', '01', '01', '00', '01', '11', '01', '01', '10', '01', '11', '00', '01', '11', '00', '01', '11', '01', '01', '01']\n",
      " True Correction: 3  ( (0, 1) )\n",
      " Transformer Predicted Correction: 3  ( (0, 1) )\n",
      " Classical Predicted Correction: 3  ( (0, 1) )\n",
      "------\n",
      "Sample 2:\n",
      " Syndrome Sequence: ['10', '10', '01', '10', '10', '10', '10', '10', '10', '00', '00', '10', '10', '10', '00', '00', '10', '11', '00', '01']\n",
      " True Correction: 1  ( (1, 0) )\n",
      " Transformer Predicted Correction: 1  ( (1, 0) )\n",
      " Classical Predicted Correction: 1  ( (1, 0) )\n",
      "------\n",
      "Sample 3:\n",
      " Syndrome Sequence: ['00', '00', '00', '10', '10', '00', '10', '00', '00', '00', '00', '00', '00', '00', '10', '00', '00', '00', '01', '00']\n",
      " True Correction: 0  ( (0, 0) )\n",
      " Transformer Predicted Correction: 0  ( (0, 0) )\n",
      " Classical Predicted Correction: 0  ( (0, 0) )\n",
      "------\n",
      "Sample 4:\n",
      " Syndrome Sequence: ['00', '11', '11', '01', '11', '01', '01', '00', '01', '01', '11', '00', '01', '11', '01', '00', '11', '11', '11', '11']\n",
      " True Correction: 3  ( (0, 1) )\n",
      " Transformer Predicted Correction: 3  ( (0, 1) )\n",
      " Classical Predicted Correction: 2  ( (1, 1) )\n",
      "------\n",
      "Sample 5:\n",
      " Syndrome Sequence: ['00', '10', '10', '10', '10', '11', '10', '10', '00', '10', '10', '10', '00', '10', '10', '10', '10', '10', '10', '00']\n",
      " True Correction: 1  ( (1, 0) )\n",
      " Transformer Predicted Correction: 1  ( (1, 0) )\n",
      " Classical Predicted Correction: 1  ( (1, 0) )\n",
      "------\n",
      "\n",
      "\n",
      "Summary:\n",
      "Independent Noise -> Transformer: 68.00%, Classical: 60.40%\n",
      "Biased Noise      -> Transformer: 97.10%, Classical: 96.60%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# -------------------------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# -------------------------------\n",
    "# Define syndrome mappings for a 3-qubit repetition (bit-flip) code.\n",
    "# -------------------------------\n",
    "ideal_syndromes = {\n",
    "    0: (0, 0),  # no error\n",
    "    1: (1, 0),  # error on qubit 1\n",
    "    2: (1, 1),  # error on qubit 2\n",
    "    3: (0, 1)   # error on qubit 3\n",
    "}\n",
    "token_map = {\n",
    "    (0, 0): 0,\n",
    "    (1, 0): 1,\n",
    "    (1, 1): 2,\n",
    "    (0, 1): 3\n",
    "}\n",
    "\n",
    "# -------------------------------\n",
    "# Data generation function.\n",
    "#\n",
    "# Two noise modes:\n",
    "#   \"independent\": each syndrome bit is flipped with probability noise_prob.\n",
    "#   \"biased\": first bit is flipped with noise_prob1, second with noise_prob2.\n",
    "#\n",
    "# Parameter enhanced_input:\n",
    "#   - If False, returns a single token per measurement (traditional).\n",
    "#   - If True, returns the two syndrome bits as a list [bit1, bit2].\n",
    "# -------------------------------\n",
    "def generate_sample(sequence_length=20, noise_mode=\"independent\", noise_prob=0.2,\n",
    "                    noise_prob1=0.5, noise_prob2=0.1, enhanced_input=False):\n",
    "    true_class = random.choice([0, 1, 2, 3])\n",
    "    ideal = ideal_syndromes[true_class]\n",
    "    sequence = []\n",
    "    for _ in range(sequence_length):\n",
    "        measured = []\n",
    "        if noise_mode == \"independent\":\n",
    "            for bit in ideal:\n",
    "                if random.random() < noise_prob:\n",
    "                    measured.append(1 - bit)\n",
    "                else:\n",
    "                    measured.append(bit)\n",
    "        elif noise_mode == \"biased\":\n",
    "            if random.random() < noise_prob1:\n",
    "                measured.append(1 - ideal[0])\n",
    "            else:\n",
    "                measured.append(ideal[0])\n",
    "            if random.random() < noise_prob2:\n",
    "                measured.append(1 - ideal[1])\n",
    "            else:\n",
    "                measured.append(ideal[1])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown noise_mode\")\n",
    "        measured = tuple(measured)\n",
    "        if enhanced_input:\n",
    "            # Return the raw two bits as integers\n",
    "            sequence.append(list(measured))\n",
    "        else:\n",
    "            token = token_map[measured]\n",
    "            sequence.append(token)\n",
    "    return sequence, true_class\n",
    "\n",
    "# -------------------------------\n",
    "# PyTorch Dataset for syndrome data.\n",
    "# -------------------------------\n",
    "class SyndromeDataset(Dataset):\n",
    "    def __init__(self, num_samples=5000, sequence_length=20, noise_mode=\"independent\",\n",
    "                 noise_prob=0.2, noise_prob1=0.5, noise_prob2=0.1,\n",
    "                 enhanced_input=False, separate_bit_embeddings=False):\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        for _ in range(num_samples):\n",
    "            seq, label = generate_sample(sequence_length, noise_mode, noise_prob, noise_prob1, noise_prob2, enhanced_input)\n",
    "            self.samples.append(seq)\n",
    "            self.labels.append(label)\n",
    "        if enhanced_input and separate_bit_embeddings:\n",
    "            # Each measurement is a list of two ints; shape: (num_samples, seq_len, 2)\n",
    "            self.samples = torch.tensor(self.samples, dtype=torch.long)\n",
    "        elif enhanced_input:\n",
    "            self.samples = torch.tensor(self.samples, dtype=torch.float32)\n",
    "        else:\n",
    "            self.samples = torch.tensor(self.samples, dtype=torch.long)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx], self.labels[idx]\n",
    "\n",
    "# -------------------------------\n",
    "# Positional Encoding Module.\n",
    "# -------------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "# -------------------------------\n",
    "# Separate Bit Embedding Module.\n",
    "# -------------------------------\n",
    "class SyndromeBitEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, bit_vocab_size=2, combine_method=\"concat\"):\n",
    "        super(SyndromeBitEmbedding, self).__init__()\n",
    "        self.embedding1 = nn.Embedding(bit_vocab_size, d_model)\n",
    "        self.embedding2 = nn.Embedding(bit_vocab_size, d_model)\n",
    "        self.combine_method = combine_method\n",
    "        if combine_method == \"concat\":\n",
    "            self.proj = nn.Linear(2 * d_model, d_model)\n",
    "        elif combine_method != \"sum\":\n",
    "            raise ValueError(\"Unknown combine_method\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, 2); each value should be int (0 or 1)\n",
    "        bit1 = x[:, :, 0].long()  # (batch, seq_len)\n",
    "        bit2 = x[:, :, 1].long()  # (batch, seq_len)\n",
    "        emb1 = self.embedding1(bit1)  # (batch, seq_len, d_model)\n",
    "        emb2 = self.embedding2(bit2)  # (batch, seq_len, d_model)\n",
    "        if self.combine_method == \"sum\":\n",
    "            out = emb1 + emb2\n",
    "        else:  # \"concat\"\n",
    "            out = torch.cat([emb1, emb2], dim=-1)  # (batch, seq_len, 2*d_model)\n",
    "            out = self.proj(out)  # (batch, seq_len, d_model)\n",
    "        return out\n",
    "\n",
    "# -------------------------------\n",
    "# Transformer-based Sequence Classifier.\n",
    "# -------------------------------\n",
    "class SyndromeTransformer(nn.Module):\n",
    "    def __init__(self, enhanced_input=False, use_separate_embeddings=False, vocab_size=4,\n",
    "                 d_model=128, nhead=8, num_layers=4, dim_feedforward=256, num_classes=4, dropout=0.1):\n",
    "        super(SyndromeTransformer, self).__init__()\n",
    "        self.enhanced_input = enhanced_input\n",
    "        self.use_separate_embeddings = use_separate_embeddings\n",
    "        if self.use_separate_embeddings:\n",
    "            # Input shape: (batch, seq_len, 2) with int values.\n",
    "            self.bit_embedding = SyndromeBitEmbedding(d_model, combine_method=\"concat\")\n",
    "        else:\n",
    "            if enhanced_input:\n",
    "                # Input shape: (batch, seq_len, 2) as float vectors.\n",
    "                self.input_projection = nn.Linear(2, d_model)\n",
    "            else:\n",
    "                # Traditional token input.\n",
    "                self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        # Extra classification head.\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.fc2 = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_separate_embeddings:\n",
    "            emb = self.bit_embedding(x)  # (batch, seq_len, d_model)\n",
    "        elif self.enhanced_input:\n",
    "            emb = self.input_projection(x)  # (batch, seq_len, d_model)\n",
    "        else:\n",
    "            emb = self.embedding(x)  # (batch, seq_len, d_model)\n",
    "        emb = self.pos_encoder(emb)\n",
    "        emb = emb.transpose(0, 1)  # (seq_len, batch, d_model)\n",
    "        out = self.transformer_encoder(emb)  # (seq_len, batch, d_model)\n",
    "        out = out.transpose(0, 1)  # (batch, seq_len, d_model)\n",
    "        pooled = out.mean(dim=1)   # Mean pooling\n",
    "        pooled = self.dropout(pooled)\n",
    "        hidden = torch.relu(self.fc1(pooled))\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc2(hidden)\n",
    "\n",
    "# -------------------------------\n",
    "# Classical decoder: Majority Vote.\n",
    "# -------------------------------\n",
    "def classical_decoder(sample, enhanced_input=False, use_separate_embeddings=False):\n",
    "    if use_separate_embeddings:\n",
    "        tokens = [token_map[tuple(x)] for x in sample.cpu().numpy()]\n",
    "    elif enhanced_input:\n",
    "        bits = [(1 if x[0] >= 0.5 else 0, 1 if x[1] >= 0.5 else 0) for x in sample.cpu().numpy()]\n",
    "        tokens = [token_map[tuple(b)] for b in bits]\n",
    "    else:\n",
    "        tokens = sample.cpu().numpy().tolist()\n",
    "    return Counter(tokens).most_common(1)[0][0]\n",
    "\n",
    "# -------------------------------\n",
    "# Training and evaluation routines.\n",
    "# -------------------------------\n",
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    return running_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def evaluate_classical(dataset, enhanced_input=False, use_separate_embeddings=False):\n",
    "    correct = 0\n",
    "    total = len(dataset)\n",
    "    for sample, label in dataset:\n",
    "        if classical_decoder(sample, enhanced_input, use_separate_embeddings) == label.item():\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "\n",
    "# -------------------------------\n",
    "# Run an experiment with early stopping and weight decay.\n",
    "# -------------------------------\n",
    "def run_experiment(noise_mode, enhanced_input, use_separate_embeddings, noise_prob=0.2, noise_prob1=0.5, noise_prob2=0.1,\n",
    "                   num_train=5000, num_val=1000, sequence_length=20, num_epochs=20,\n",
    "                   d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.1, weight_decay=1e-5, patience=5):\n",
    "    mode_str = \"Separate Bit Embeddings\" if use_separate_embeddings else (\"Enhanced\" if enhanced_input else \"Traditional\")\n",
    "    print(f\"=== Experiment ({mode_str}): noise_mode = {noise_mode} ===\")\n",
    "\n",
    "    train_dataset = SyndromeDataset(num_samples=num_train, sequence_length=sequence_length,\n",
    "                                    noise_mode=noise_mode, noise_prob=noise_prob,\n",
    "                                    noise_prob1=noise_prob1, noise_prob2=noise_prob2,\n",
    "                                    enhanced_input=enhanced_input, separate_bit_embeddings=use_separate_embeddings)\n",
    "    val_dataset = SyndromeDataset(num_samples=num_val, sequence_length=sequence_length,\n",
    "                                  noise_mode=noise_mode, noise_prob=noise_prob,\n",
    "                                  noise_prob1=noise_prob1, noise_prob2=noise_prob2,\n",
    "                                  enhanced_input=enhanced_input, separate_bit_embeddings=use_separate_embeddings)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = SyndromeTransformer(enhanced_input=enhanced_input, use_separate_embeddings=use_separate_embeddings,\n",
    "                                vocab_size=4, d_model=d_model, nhead=nhead, num_layers=num_layers,\n",
    "                                dim_feedforward=dim_feedforward, num_classes=4, dropout=dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer, device)\n",
    "        val_acc = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}, Transformer Val Accuracy: {val_acc*100:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1} (best epoch: {best_epoch+1}, best accuracy: {best_val_acc*100:.2f}%)\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "\n",
    "    transformer_acc = evaluate_model(model, val_loader, device)\n",
    "    classical_acc = evaluate_classical(val_dataset, enhanced_input, use_separate_embeddings)\n",
    "    print(f\"\\nFinal Transformer Val Accuracy: {transformer_acc*100:.2f}%\")\n",
    "    print(f\"Classical Decoder (Majority Vote) Val Accuracy: {classical_acc*100:.2f}%\")\n",
    "\n",
    "    print(\"\\nTesting examples:\")\n",
    "    for i in range(5):\n",
    "        sample, true_label = val_dataset[i]\n",
    "        sample_tensor = sample.unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(sample_tensor)\n",
    "            _, pred_transformer = torch.max(output, 1)\n",
    "        pred_classical = classical_decoder(sample, enhanced_input, use_separate_embeddings)\n",
    "        if use_separate_embeddings:\n",
    "            decoded_seq = [\"\".join(str(x) for x in vec.tolist()) for vec in sample]\n",
    "        elif enhanced_input:\n",
    "            decoded_seq = [\"\".join(str(int(b)) for b in vec.tolist()) for vec in sample]\n",
    "        else:\n",
    "            inv_token_map = {v: k for k, v in token_map.items()}\n",
    "            decoded_seq = [\"\".join(str(b) for b in inv_token_map[int(token)]) for token in sample.tolist()]\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(\" Syndrome Sequence:\", decoded_seq)\n",
    "        print(\" True Correction:\", true_label.item(), \" (\", ideal_syndromes[true_label.item()],\")\")\n",
    "        print(\" Transformer Predicted Correction:\", pred_transformer.item(), \" (\", ideal_syndromes[pred_transformer.item()],\")\")\n",
    "        print(\" Classical Predicted Correction:\", pred_classical, \" (\", ideal_syndromes[pred_classical],\")\")\n",
    "        print(\"------\")\n",
    "    print(\"\\n\")\n",
    "    return transformer_acc, classical_acc\n",
    "\n",
    "# -------------------------------\n",
    "# Main routine: Run experiments.\n",
    "# -------------------------------\n",
    "def main():\n",
    "    # Use enhanced input with separate bit embeddings.\n",
    "    enhanced_input = True\n",
    "    use_separate_embeddings = True\n",
    "\n",
    "    # Experiment 1: Independent noise\n",
    "    transformer_acc_ind, classical_acc_ind = run_experiment(\n",
    "        noise_mode=\"independent\",\n",
    "        enhanced_input=enhanced_input,\n",
    "        use_separate_embeddings=use_separate_embeddings,\n",
    "        noise_prob=0.4,\n",
    "        num_train=5000,\n",
    "        num_val=1000,\n",
    "        sequence_length=20,\n",
    "        num_epochs=20,\n",
    "        d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.1,\n",
    "        weight_decay=1e-5,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # Experiment 2: Biased noise\n",
    "    transformer_acc_bias, classical_acc_bias = run_experiment(\n",
    "        noise_mode=\"biased\",\n",
    "        enhanced_input=enhanced_input,\n",
    "        use_separate_embeddings=use_separate_embeddings,\n",
    "        noise_prob=0.2,      # not used in biased mode\n",
    "        noise_prob1=0.3,     # high flip probability for first bit\n",
    "        noise_prob2=0.1,     # low flip probability for second bit\n",
    "        num_train=5000,\n",
    "        num_val=1000,\n",
    "        sequence_length=20,\n",
    "        num_epochs=20,\n",
    "        d_model=128, nhead=8, num_layers=4, dim_feedforward=256, dropout=0.1,\n",
    "        weight_decay=1e-5,\n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    print(\"Summary:\")\n",
    "    print(f\"Independent Noise -> Transformer: {transformer_acc_ind*100:.2f}%, Classical: {classical_acc_ind*100:.2f}%\")\n",
    "    print(f\"Biased Noise      -> Transformer: {transformer_acc_bias*100:.2f}%, Classical: {classical_acc_bias*100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIC4lif9fR98"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
